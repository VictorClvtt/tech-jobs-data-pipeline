# tech-jobs-data-pipeline

## Sum√°rio

-   [Sobre](#sobre)
-   [Setup](#setup)

## Sobre

Este projeto consiste em uma **pipeline de dados** respons√°vel por
realizar **web scraping** de vagas de emprego na √°rea de tecnologia a
partir de tr√™s fontes diferentes: **NerdIn**, **Programathor** e
**Vagas.com**.

Ap√≥s a etapa de extra√ß√£o, os dados passam por m√∫ltiplas fases de
**processamento e transforma√ß√£o** at√© atingirem o formato ideal para a
finalidade proposta: um **site agregador de vagas de tecnologia**. Em
cada etapa da pipeline, os dados s√£o armazenados em **camadas distintas
de um bucket de armazenamento**, garantindo **hist√≥rico**,
**rastreabilidade** e **reprocessamento confi√°vel** das informa√ß√µes.

Toda essa pipeline viabiliza a exist√™ncia de um **aplicativo em
Streamlit**, que permite a navega√ß√£o pelas vagas coletadas, oferecendo
recursos como **filtros personalizados**, visualiza√ß√£o de informa√ß√µes
detalhadas e **acesso direto ao link original** de cada vaga.

Diagrama explicando visualmente os componentes e funcionamento do projeto:
![](./docs/diagrams/project_diagram.png)

Na camada Gold s√£o gravados os dados modelados para poderem ser consumidos pelo App, seguindo a seguinte estrutura:
![](./docs/diagrams/model_erd_diagram.png)

O app est√° hospedado no link https://tech-jobs-data-pipeline-lt8l7hzcnhwazy5jztldst.streamlit.app/ e possui um conjunto de dados correspondente √† **extra√ß√£o de um √∫nico
dia**, apenas para **demonstra√ß√£o do app**.

------------------------------------------------------------------------

## Setup

Independente se o projeto ser√° executado **manualmente** ou **pelo
Airflow**, o primeiro passo para a instala√ß√£o do projeto √© baixar o
reposit√≥rio executando o seguinte comando no diret√≥rio onde desejar que
ele fique:

``` bash
git clone https://github.com/VictorClvtt/tech-jobs-data-pipeline.git
```

Com o reposit√≥rio baixado, basta decidir **como deseja executar o
projeto** e seguir as instru√ß√µes de uma das op√ß√µes a seguir.

------------------------------------------------------------------------

### Execu√ß√£o Manual

Antes de se preocupar com as depend√™ncias necess√°rias para a **execu√ß√£o
do c√≥digo do projeto**, √© necess√°rio satisfazer duas **depend√™ncias de
ambiente**:

-   **JDK 17** (compat√≠vel com Spark), que pode ser instalada com o
    comando:

    ``` bash
    sudo pacman -S jdk17-openjdk
    ```

-   **MinIO** (utilizado como bucket de armazenamento), que pode ser
    executado localmente com Docker por meio do comando:

    ``` bash
    docker run -d \
      --name minio \
      --hostname minio \
      -p 9000:9000 \
      -p 9001:9001 \
      -e MINIO_ROOT_USER=admin \
      -e MINIO_ROOT_PASSWORD=admin123 \
      -v minio-data:/data \
      --restart unless-stopped \
      minio/minio:latest server /data --console-address ":9001"
    ```

*(assumindo que voc√™ j√° tenha o Docker instalado)*

------------------------------------------------------------------------

Para executar o c√≥digo do projeto manualmente, primeiramente crie um
**virtual environment** do Python no diret√≥rio raiz do projeto:

``` bash
python -m venv .venv
```

Em seguida, ative o virtual env:

``` bash
source .venv/bin/activate
```

Depois disso, instale as bibliotecas necess√°rias listadas em
[`requirements.txt`](./requirements.txt):

``` bash
pip install -r requirements.txt
```

Ap√≥s a instala√ß√£o de todas as bibliotecas, o ambiente estar√° pronto para
a execu√ß√£o manual dos scripts do projeto.

------------------------------------------------------------------------

√â importante ressaltar que os scripts foram feitos para serem executados
como **m√≥dulos**, a partir do diret√≥rio raiz do projeto. Alguns exemplos
de execu√ß√£o:

``` bash
python -m src.bronze.run_bronze
python -m src.silver.run_silver_normalizer
python -m src.gold.run_gold
```

> Apesar de todas as etapas da pipeline terem sido modularizadas, nem
> todas foram programadas para serem executadas de forma simples via
> linha de comando como mostrado acima. Ainda assim, √© perfeitamente
> poss√≠vel executar manualmente as principais etapas da pipeline:
> **bronze**, **silver** e **gold**.

------------------------------------------------------------------------

Para executar o **app Streamlit**
[`streamlit_app.py`](./src/app/streamlit_app.py), utilize o mesmo
virtual env e execute:

``` bash
streamlit run src/app/streamlit_app.py
```

------------------------------------------------------------------------

Tamb√©m √© importante **renomear manualmente** o arquivo
[`.env.example`](./.env.example) para `.env`, para que os scripts
consigam ler as vari√°veis de ambiente. Caso necess√°rio, os valores das
vari√°veis podem ser alterados sem problemas.

------------------------------------------------------------------------

**OBS**:\
O app Streamlit possui um par√¢metro **hard coded** em uma constante na
**linha 29** do arquivo
[`streamlit_app.py`](./src/app/streamlit_app.py). Esse par√¢metro define
a fonte de dados utilizada pelo app.

Valor atual:

``` python
DATA_SOURCE = "local"
```

Esse valor faz com que o app leia os dados de um **cat√°logo DuckDB**
previamente criado para demonstra√ß√£o, independente dos dados presentes
no bucket.

Para que o app leia **diretamente os dados do MinIO**, altere para:

``` python
DATA_SOURCE = "minio"
```

------------------------------------------------------------------------

### Execu√ß√£o via Airflow/Docker

Para a execu√ß√£o da pipeline utilizando **Airflow via Docker**, √©
necess√°rio ter os seguintes pr√©-requisitos instalados na m√°quina:

-   **Astro CLI**
-   **Docker**
-   **Docker Compose**

Com todos os requisitos instalados, execute o **script de setup** do
ambiente Docker Compose para o Airflow.

Antes disso, certifique-se de atribuir permiss√£o de execu√ß√£o ao script:

``` bash
chmod +x airflow_setup.sh
```

Em seguida, execute o script:

``` bash
./airflow_setup.sh <nome-do-data-lake>
```

O par√¢metro entre `<>` corresponde ao **nome do data lake**, que ser√°
utilizado durante a execu√ß√£o do script e propagado para o arquivo `.env`
do Airflow, sendo consumido pelas tasks.

Ap√≥s a conclus√£o do script, o ambiente estar√° pronto para executar a
pipeline conforme o agendamento definido na DAG:

üëâ [`tech_jobs_dag.py`](./airflow/dags/tech_jobs_dag.py)

------------------------------------------------------------------------

Caso haja interesse, √© poss√≠vel alterar os valores das vari√°veis
presentes no arquivo [`.env.example`](./airflow/.env.example) **antes**
da execu√ß√£o do setup, sem problemas.

------------------------------------------------------------------------

**OBS**:\
O app Streamlit presente no ambiente do Airflow
([`streamlit_app.py`](./airflow/include/src/app/streamlit_app.py))
tamb√©m possui o mesmo par√¢metro **hard coded** na linha 29.

Valor atual:

``` python
DATA_SOURCE = "local"
```

Para alterar a leitura para os dados do bucket MinIO:

``` python
DATA_SOURCE = "minio"
```
